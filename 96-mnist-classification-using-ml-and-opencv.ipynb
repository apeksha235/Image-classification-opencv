{"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Number Detection using OpenCV and Decision Trees \n*MNIST dataset*\nYou can access the dataset here: [Kaggle MNIST dataset](https://www.kaggle.com/competitions/digit-recognizer/data)","metadata":{"id":"8VSMSdsxavx2"}},{"cell_type":"markdown","source":"> This notebook uses the MNIST number classification dataset to recognizer numbers with the help of OpenCV and Random Forest Classifier. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_digits\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n%matplotlib inline","metadata":{"id":"WV1DJZSgatge","execution":{"iopub.status.busy":"2022-09-28T18:59:11.594179Z","iopub.execute_input":"2022-09-28T18:59:11.594689Z","iopub.status.idle":"2022-09-28T18:59:12.833747Z","shell.execute_reply.started":"2022-09-28T18:59:11.594585Z","shell.execute_reply":"2022-09-28T18:59:12.832480Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"> Reading the data","metadata":{"id":"78ukqewsqxpj"}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntrain.shape","metadata":{"id":"SuvozQCw8TLY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e2a32cd-a12c-4566-81b1-598a71f3f4a5","execution":{"iopub.status.busy":"2022-09-28T18:59:12.836017Z","iopub.execute_input":"2022-09-28T18:59:12.836566Z","iopub.status.idle":"2022-09-28T18:59:16.973944Z","shell.execute_reply.started":"2022-09-28T18:59:12.836519Z","shell.execute_reply":"2022-09-28T18:59:16.972645Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(42000, 785)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Explaination of the MNIST dataset\n> - MNIST is a famous dataset that consists of handwritten numbers for classification. \n-It consists of 785 columns in total where 1 of it is the `label` (the number that represents the specific row) and the rest 784 columns are `pixels` of the image. \n<br></br>\n- **Why 784 pixels of the image?** \n- The images that we are using for classification of numbers are of 28x28 pixels. Something like this: \n<br></br>\n<img src=\"http://i.imgur.com/rwMpFVu.png\" height=\"200\"></img>\n\n> - To represent this 28x28 matrix in a dataset will be incovenient. Hence we turn this into a `single row` of 784 (28*28) containing all the digits shown in the image above. Like this- \n<br></br>\n<img src=\"https://static.wixstatic.com/media/34bab5_2bf56349ff884c49bfe06edfb33eb1e9~mv2.png/v1/fill/w_910,h_267,al_c/34bab5_2bf56349ff884c49bfe06edfb33eb1e9~mv2.png\" height=\"200\"  width=\"500\"></img>\n\n\n> -Hence we turn the 2D matrix to a 1D matrix. This is called `Row Flattening`\n- This 1D matrix is then added as a single row in our MNIST dataset widht 784 columns. \n- The **785th** column depicts the number that is represented by those 784 pixels.\n","metadata":{"id":"7aO1hBHqq1PB"}},{"cell_type":"markdown","source":"### Entire model: \n<br></br>\n<img src=\"http://gpucomputing.shef.ac.uk/static/img/intro_dl_sharc_dgx1/mnist_simple.png\" height=\"200\" ></img>","metadata":{"id":"w63oo-ZAuZTI"}},{"cell_type":"code","source":"y=train['label']\nX=train.drop('label',axis=1)\ntrain.head()","metadata":{"id":"MNE2evf-9gaL","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"2f03fbef-bd75-46ce-8372-008215466b98","execution":{"iopub.status.busy":"2022-09-28T18:59:16.975151Z","iopub.execute_input":"2022-09-28T18:59:16.975500Z","iopub.status.idle":"2022-09-28T18:59:17.118607Z","shell.execute_reply.started":"2022-09-28T18:59:16.975456Z","shell.execute_reply":"2022-09-28T18:59:17.117430Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- In the MNIST dataset shown above, column `label` is the number that we have to predict while the rest of the columns are the pixels of that number. ","metadata":{"id":"rJrAqP0YuvDC"}},{"cell_type":"markdown","source":"### Training the model","metadata":{"id":"O3df5OvnvF63"}},{"cell_type":"markdown","source":"> We will be splitting our training data, 80% for training our model and 20% for testing it's accuracy","metadata":{"id":"SWe3A8aNvKQr"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","metadata":{"id":"IChtnP8ArwS3","execution":{"iopub.status.busy":"2022-09-28T18:59:17.121368Z","iopub.execute_input":"2022-09-28T18:59:17.122172Z","iopub.status.idle":"2022-09-28T18:59:17.443407Z","shell.execute_reply.started":"2022-09-28T18:59:17.122130Z","shell.execute_reply":"2022-09-28T18:59:17.441915Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"> The model input X requires a `Float` value while the output Y needs to be in the integer form (since integer values are predicted).","metadata":{"id":"aICktzOCvb8Z"}},{"cell_type":"code","source":"X_train = np.asarray(X_train).astype(np.float32)\ny_train= np.asarray(y_train).astype(np.int64)\nX_test = np.asarray(X_test).astype(np.float32)","metadata":{"id":"Mjzz3qquM1lJ","execution":{"iopub.status.busy":"2022-09-28T18:59:17.445000Z","iopub.execute_input":"2022-09-28T18:59:17.445720Z","iopub.status.idle":"2022-09-28T18:59:17.520225Z","shell.execute_reply.started":"2022-09-28T18:59:17.445675Z","shell.execute_reply":"2022-09-28T18:59:17.518775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"> `Rtrees` is a class of OpenCV that helps us to implement Random Forest algorithm","metadata":{"id":"xshnxnDZwWNj"}},{"cell_type":"code","source":"import cv2\nrtree = cv2.ml.RTrees_create()","metadata":{"id":"fDQAG4U_NHq-","execution":{"iopub.status.busy":"2022-09-28T18:59:17.521830Z","iopub.execute_input":"2022-09-28T18:59:17.522300Z","iopub.status.idle":"2022-09-28T18:59:17.752142Z","shell.execute_reply.started":"2022-09-28T18:59:17.522255Z","shell.execute_reply":"2022-09-28T18:59:17.751019Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"> Hyperparameter tuning for our model","metadata":{"id":"nyJ_QGf510GN"}},{"cell_type":"code","source":"num_trees = 100 ## No. of trees for training our model\neps = 0.01 ## Stops the algorithm if this accuracy is reached (error rate=0.01)\ncriteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n            num_trees, eps)\nrtree.setTermCriteria(criteria) ## defines these criterias for the model rtree","metadata":{"id":"X1PCuWWbNL6-","execution":{"iopub.status.busy":"2022-09-28T18:59:17.753427Z","iopub.execute_input":"2022-09-28T18:59:17.754555Z","iopub.status.idle":"2022-09-28T18:59:17.761510Z","shell.execute_reply.started":"2022-09-28T18:59:17.754515Z","shell.execute_reply":"2022-09-28T18:59:17.760157Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> Sample tree for reference \n<br></br>\n<img src=\"https://forum.huawei.com/enterprise/en/data/attachment/forum/202103/24/190400o09x7rhnnhy2yon7.png?1.png\" height=\"300\"><img>","metadata":{"id":"9a6V2vG83XGK"}},{"cell_type":"code","source":"rtree.setMaxCategories(len(np.unique(y_train))) ## max categories in our case: 10 (since numbers from 0 to 9 can be predicted)","metadata":{"id":"FuQQ7iTRnJeo","execution":{"iopub.status.busy":"2022-09-28T18:59:17.763162Z","iopub.execute_input":"2022-09-28T18:59:17.764055Z","iopub.status.idle":"2022-09-28T18:59:17.775671Z","shell.execute_reply.started":"2022-09-28T18:59:17.764017Z","shell.execute_reply":"2022-09-28T18:59:17.774484Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"rtree.setMinSampleCount(2) ## If the number of samples in a node is less than this parameter then the node will not be split.\n## As shown in the image above, minimum number of samples required to split a node is 2. ","metadata":{"id":"D7yQsGoFNQ0B","execution":{"iopub.status.busy":"2022-09-28T18:59:17.777088Z","iopub.execute_input":"2022-09-28T18:59:17.777547Z","iopub.status.idle":"2022-09-28T18:59:17.785319Z","shell.execute_reply.started":"2022-09-28T18:59:17.777505Z","shell.execute_reply":"2022-09-28T18:59:17.784023Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"rtree.setMaxDepth(1000) ## Setting a maximum depth above which the tree won't be split. (Levels of the decision tree)","metadata":{"id":"ZtwA3Ct1NSj3","execution":{"iopub.status.busy":"2022-09-28T18:59:17.789600Z","iopub.execute_input":"2022-09-28T18:59:17.789996Z","iopub.status.idle":"2022-09-28T18:59:17.794846Z","shell.execute_reply.started":"2022-09-28T18:59:17.789960Z","shell.execute_reply":"2022-09-28T18:59:17.793720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"> After our hyperparameter tuning, we will be training our model.","metadata":{"id":"g0uPE3kZ4eCK"}},{"cell_type":"code","source":"rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train)","metadata":{"id":"V0TAmYjnNWZD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09dc0310-5934-4135-f3e4-73f17d5208ee","execution":{"iopub.status.busy":"2022-09-28T18:59:17.796744Z","iopub.execute_input":"2022-09-28T18:59:17.797090Z","iopub.status.idle":"2022-09-28T19:00:30.300030Z","shell.execute_reply.started":"2022-09-28T18:59:17.797058Z","shell.execute_reply":"2022-09-28T19:00:30.298815Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"rtree.getMaxDepth() ## Max depth of our tree after training our model is 25","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKQbM_cOUQTZ","outputId":"cdd97346-7198-41ac-bc86-f7e9b326256b","execution":{"iopub.status.busy":"2022-09-28T19:00:30.301379Z","iopub.execute_input":"2022-09-28T19:00:30.301776Z","iopub.status.idle":"2022-09-28T19:00:30.308815Z","shell.execute_reply.started":"2022-09-28T19:00:30.301742Z","shell.execute_reply":"2022-09-28T19:00:30.307783Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}]},{"cell_type":"code","source":"a , y_hat = rtree.predict(X_test) ## Predicting the 20% of the data that we took for testing","metadata":{"id":"n8xTbajvV1VD","execution":{"iopub.status.busy":"2022-09-28T19:00:30.310947Z","iopub.execute_input":"2022-09-28T19:00:30.311278Z","iopub.status.idle":"2022-09-28T19:00:30.902989Z","shell.execute_reply.started":"2022-09-28T19:00:30.311248Z","shell.execute_reply":"2022-09-28T19:00:30.901680Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(a)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auOwmubpA0yE","outputId":"ee0a7987-ef8d-44e8-f1f4-b61ac1666ad8","execution":{"iopub.status.busy":"2022-09-28T19:00:30.904270Z","iopub.execute_input":"2022-09-28T19:00:30.904657Z","iopub.status.idle":"2022-09-28T19:00:30.912018Z","shell.execute_reply.started":"2022-09-28T19:00:30.904623Z","shell.execute_reply":"2022-09-28T19:00:30.910649Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_hat) ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DazLmLRV3k-","outputId":"02a71b56-9376-443e-a8e8-168eb8511563","execution":{"iopub.status.busy":"2022-09-28T19:00:30.913844Z","iopub.execute_input":"2022-09-28T19:00:30.914304Z","iopub.status.idle":"2022-09-28T19:00:30.925351Z","shell.execute_reply.started":"2022-09-28T19:00:30.914260Z","shell.execute_reply":"2022-09-28T19:00:30.924040Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.9657142857142857"},"metadata":{}}]},{"cell_type":"markdown","source":"> Hence we got an accuracy of 96%, we can improve this by tuning the number of trees, depth, sample count etc. ","metadata":{"id":"8wvUk8rZ4wcu"}},{"cell_type":"markdown","source":"## Testing with actual images","metadata":{"id":"-NsSt1Mq5AQZ"}},{"cell_type":"markdown","source":"> - Now we will be using an actual image for testing our model, to see how well it predicts. \n> - Since our dataset is for 28x28 pixel images, we resize our image to 28x28","metadata":{"id":"hTSuireV5GAn"}},{"cell_type":"code","source":"file = r'../input/mnistasjpg/testSample/testSample/img_142.jpg'\ntest_image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n\n# Format Image\nimg_resized = cv2.resize(test_image, (28,28), interpolation=cv2.INTER_LINEAR)\nplt.imshow(img_resized, cmap='gray') ## Displaying the image that we have to predict ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"s4t47pXy-tLS","outputId":"65e010e1-d81b-4c01-8a9e-6fe08f43dabf","execution":{"iopub.status.busy":"2022-09-28T19:01:29.254464Z","iopub.execute_input":"2022-09-28T19:01:29.254897Z","iopub.status.idle":"2022-09-28T19:01:29.421275Z","shell.execute_reply.started":"2022-09-28T19:01:29.254862Z","shell.execute_reply":"2022-09-28T19:01:29.420367Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f752a58da90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"> - Now we flatten our image of 28x28 to convert it to a single vector of size 784 so that it fits our model. \n- Our model only accepts a single row of 784 features.","metadata":{"id":"LtA-M2y57DRU"}},{"cell_type":"code","source":"img_resized=img_resized.astype('float32')\nimg_resized -= img_resized.mean(axis=0)\nimg_resized = img_resized.flatten().reshape(1,784)","metadata":{"id":"idyQF2kX_voR","execution":{"iopub.status.busy":"2022-09-28T19:01:31.426456Z","iopub.execute_input":"2022-09-28T19:01:31.427427Z","iopub.status.idle":"2022-09-28T19:01:31.432413Z","shell.execute_reply.started":"2022-09-28T19:01:31.427388Z","shell.execute_reply":"2022-09-28T19:01:31.431568Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"> - Predicting our image ","metadata":{"id":"4FMHMGMl7sOe"}},{"cell_type":"code","source":"_, pred = rtree.predict(img_resized)","metadata":{"id":"ykaJKVzrHKYD","execution":{"iopub.status.busy":"2022-09-28T19:01:32.971628Z","iopub.execute_input":"2022-09-28T19:01:32.972318Z","iopub.status.idle":"2022-09-28T19:01:32.978645Z","shell.execute_reply.started":"2022-09-28T19:01:32.972269Z","shell.execute_reply":"2022-09-28T19:01:32.977386Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukKgdpw7HeUR","outputId":"2fa3ad42-27dd-4a86-e9b2-a8a62e63e351","execution":{"iopub.status.busy":"2022-09-28T19:01:34.942927Z","iopub.execute_input":"2022-09-28T19:01:34.943326Z","iopub.status.idle":"2022-09-28T19:01:34.950448Z","shell.execute_reply.started":"2022-09-28T19:01:34.943293Z","shell.execute_reply":"2022-09-28T19:01:34.949552Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([[3.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"zQUBi2F3zt-b"},"execution_count":null,"outputs":[]}]}